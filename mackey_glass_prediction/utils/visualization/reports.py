"""
Gera√ß√£o de relat√≥rios abrangentes
"""
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from datetime import datetime
import os
from io import StringIO

from .utils import ensure_output_dir, print_save_message, validate_and_clean_metrics, format_metric_value
from .basic_plots import plot_training_history, plot_predictions, plot_metrics_comparison
from .distribution_analysis import plot_qq_analysis, plot_cdf_comparison, plot_pdf_comparison
from .statistical_tests import plot_ks_test_analysis, plot_autocorrelation_analysis
from .comparison_plots import plot_models_comparison_overview, plot_performance_radar


def generate_comprehensive_report(results_dict, output_dir, model_name=None):
    """
    Gera relat√≥rio abrangente com todas as an√°lises dispon√≠veis
    
    Args:
        results_dict: Dicion√°rio com resultados dos modelos (ou resultados de um √∫nico modelo)
        output_dir: Diret√≥rio para salvar o relat√≥rio
        model_name: Nome do modelo (usado quando results_dict cont√©m dados de um √∫nico modelo)
    
    Returns:
        dict: Dicion√°rio com caminhos dos arquivos gerados
    """
    ensure_output_dir(output_dir)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    generated_files = {}
    
    # Determinar se √© an√°lise de um modelo ou compara√ß√£o
    if model_name and isinstance(results_dict, dict) and 'actuals' in results_dict:
        # An√°lise de um √∫nico modelo
        single_model_results = {model_name: results_dict}
        report_type = "single"
        report_title = f"Relat√≥rio Abrangente - {model_name}"
    else:
        # Compara√ß√£o de m√∫ltiplos modelos
        single_model_results = results_dict
        report_type = "comparison"
        report_title = "Relat√≥rio Comparativo de Modelos"
    
    print(f"\nüîÑ Gerando relat√≥rio abrangente ({report_type})...")
    print(f"üìÅ Diret√≥rio de sa√≠da: {output_dir}")
    
    # ========== AN√ÅLISE GERAL ==========
    if report_type == "comparison":
        print("\nüìä Gerando vis√£o geral comparativa...")
        overview_path = os.path.join(output_dir, f"01_visao_geral_{timestamp}.png")
        try:
            plot_models_comparison_overview(single_model_results, save_path=overview_path)
            generated_files['overview'] = overview_path
        except Exception as e:
            print(f"‚ö†Ô∏è Erro na vis√£o geral: {e}")
    
    # ========== GR√ÅFICOS B√ÅSICOS ==========
    print("\nüìà Gerando gr√°ficos b√°sicos...")
    
    # Para cada modelo individualmente
    for i, (model_name, results) in enumerate(single_model_results.items(), 1):
        model_prefix = f"{i:02d}_{model_name.replace(' ', '_')}"
        
        # Hist√≥rico de treinamento
        if 'train_losses' in results and 'val_losses' in results:
            training_path = os.path.join(output_dir, f"{model_prefix}_treinamento_{timestamp}.png")
            try:
                plot_training_history(results['train_losses'], results['val_losses'], 
                                    save_path=training_path, title=f"Hist√≥rico de Treinamento - {model_name}")
                generated_files[f'training_{model_name}'] = training_path
            except Exception as e:
                print(f"‚ö†Ô∏è Erro no hist√≥rico de treinamento para {model_name}: {e}")
        
        # Predi√ß√µes vs Reais
        if 'actuals' in results and 'predictions' in results:
            predictions_path = os.path.join(output_dir, f"{model_prefix}_predicoes_{timestamp}.png")
            try:
                plot_predictions(results['actuals'], results['predictions'], 
                               save_path=predictions_path, title=f"Predi√ß√µes vs Valores Reais - {model_name}")
                generated_files[f'predictions_{model_name}'] = predictions_path
            except Exception as e:
                print(f"‚ö†Ô∏è Erro nas predi√ß√µes para {model_name}: {e}")
    
    # ========== AN√ÅLISES ESTAT√çSTICAS ==========
    print("\nüî¨ Gerando an√°lises estat√≠sticas...")
    
    for i, (model_name, results) in enumerate(single_model_results.items(), 1):
        if 'actuals' not in results or 'predictions' not in results:
            continue
            
        model_prefix = f"{i:02d}_{model_name.replace(' ', '_')}"
        
        # QQ-Plot
        qq_path = os.path.join(output_dir, f"{model_prefix}_qq_plot_{timestamp}.png")
        try:
            plot_qq_analysis(results['actuals'], results['predictions'], 
                           save_path=qq_path, title=f"QQ-Plot - {model_name}")
            generated_files[f'qq_{model_name}'] = qq_path
        except Exception as e:
            print(f"‚ö†Ô∏è Erro no Q-Q plot para {model_name}: {e}")
        
        # An√°lise de distribui√ß√µes (CDF e FDP)
        cdf_path = os.path.join(output_dir, f"{model_prefix}_cdf_{timestamp}.png")
        try:
            plot_cdf_comparison(results['actuals'], results['predictions'], 
                              save_path=cdf_path, title=f"Compara√ß√£o FDA - {model_name}")
            generated_files[f'cdf_{model_name}'] = cdf_path
        except Exception as e:
            print(f"‚ö†Ô∏è Erro na an√°lise CDF para {model_name}: {e}")
        
        pdf_path = os.path.join(output_dir, f"{model_prefix}_pdf_{timestamp}.png")
        try:
            plot_pdf_comparison(results['actuals'], results['predictions'], 
                              save_path=pdf_path, title=f"Compara√ß√£o FDP - {model_name}")
            generated_files[f'pdf_{model_name}'] = pdf_path
        except Exception as e:
            print(f"‚ö†Ô∏è Erro na an√°lise FDP para {model_name}: {e}")
        
        # Teste de Kolmogorov-Smirnov
        ks_path = os.path.join(output_dir, f"{model_prefix}_ks_test_{timestamp}.png")
        try:
            plot_ks_test_analysis(results['actuals'], results['predictions'], 
                                 save_path=ks_path, title=f"Teste Kolmogorov-Smirnov - {model_name}")
            generated_files[f'ks_{model_name}'] = ks_path
        except Exception as e:
            print(f"‚ö†Ô∏è Erro no teste KS para {model_name}: {e}")
        
        # An√°lise de autocorrela√ß√£o (se existir s√©rie temporal)
        if 'series' in results:
            autocorr_path = os.path.join(output_dir, f"{model_prefix}_autocorrelacao_{timestamp}.png")
            try:
                plot_autocorrelation_analysis(results['series'], 
                                             save_path=autocorr_path, title=f"An√°lise de Autocorrela√ß√£o - {model_name}")
                generated_files[f'autocorr_{model_name}'] = autocorr_path
            except Exception as e:
                print(f"‚ö†Ô∏è Erro na an√°lise de autocorrela√ß√£o para {model_name}: {e}")
    
    # ========== AN√ÅLISES COMPARATIVAS ==========
    if report_type == "comparison" and len(single_model_results) > 1:
        print("\nüÜö Gerando an√°lises comparativas...")
        
        # Gr√°fico radar
        radar_path = os.path.join(output_dir, f"99_radar_performance_{timestamp}.png")
        try:
            plot_performance_radar(single_model_results, save_path=radar_path)
            generated_files['radar'] = radar_path
        except Exception as e:
            print(f"‚ö†Ô∏è Erro no gr√°fico radar: {e}")
        
        # Tabela de m√©tricas
        metrics_path = os.path.join(output_dir, f"99_tabela_metricas_{timestamp}.png")
        try:
            plot_metrics_comparison(single_model_results, save_path=metrics_path)
            generated_files['metrics_table'] = metrics_path
        except Exception as e:
            print(f"‚ö†Ô∏è Erro na tabela de m√©tricas: {e}")
    
    # ========== RELAT√ìRIO TEXTUAL ==========
    print("\nüìÑ Gerando relat√≥rio textual...")
    text_report_path = os.path.join(output_dir, f"relatorio_textual_{timestamp}.txt")
    try:
        generate_text_report(single_model_results, text_report_path, report_type)
        generated_files['text_report'] = text_report_path
    except Exception as e:
        print(f"‚ö†Ô∏è Erro no relat√≥rio textual: {e}")
    
    # ========== SUM√ÅRIO HTML ==========
    print("\nüåê Gerando sum√°rio HTML...")
    html_report_path = os.path.join(output_dir, f"relatorio_html_{timestamp}.html")
    try:
        generate_html_report(single_model_results, generated_files, html_report_path, report_type)
        generated_files['html_report'] = html_report_path
    except Exception as e:
        print(f"‚ö†Ô∏è Erro no relat√≥rio HTML: {e}")
    
    print(f"\n‚úÖ Relat√≥rio abrangente gerado com sucesso!")
    print(f"üìÅ {len(generated_files)} arquivos gerados em: {output_dir}")
    
    return generated_files


def generate_text_report(results_dict, save_path, report_type="comparison"):
    """
    Gera relat√≥rio textual detalhado
    
    Args:
        results_dict: Dicion√°rio com resultados dos modelos
        save_path: Caminho para salvar o relat√≥rio
        report_type: Tipo do relat√≥rio ("single" ou "comparison")
    """
    from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
    from scipy import stats
    
    clean_results = validate_and_clean_metrics(results_dict)
    
    with open(save_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("RELAT√ìRIO DETALHADO DE AN√ÅLISE DE MODELOS\n")
        f.write("=" * 80 + "\n")
        f.write(f"Gerado em: {datetime.now().strftime('%d/%m/%Y √†s %H:%M:%S')}\n")
        f.write(f"Tipo de an√°lise: {'Compara√ß√£o de m√∫ltiplos modelos' if report_type == 'comparison' else 'An√°lise de modelo √∫nico'}\n")
        f.write(f"N√∫mero de modelos analisados: {len(clean_results)}\n\n")
        
        # ========== RESUMO EXECUTIVO ==========
        f.write("RESUMO EXECUTIVO\n")
        f.write("-" * 50 + "\n\n")
        
        if not clean_results:
            f.write("‚ùå ERRO: Dados insuficientes para an√°lise.\n")
            return
        
        # Calcular m√©tricas para todos os modelos
        model_metrics = {}
        for model_name, results in clean_results.items():
            if 'actuals' in results and 'predictions' in results:
                actuals = np.array(results['actuals']).flatten()
                predictions = np.array(results['predictions']).flatten()
                
                # M√©tricas de performance
                r2 = r2_score(actuals, predictions)
                rmse = np.sqrt(mean_squared_error(actuals, predictions))
                mae = mean_absolute_error(actuals, predictions)
                mape = np.mean(np.abs((actuals - predictions) / actuals)) * 100 if np.all(actuals != 0) else np.nan
                
                # An√°lise de res√≠duos
                residuals = predictions - actuals
                residuals_mean = np.mean(residuals)
                residuals_std = np.std(residuals)
                residuals_skew = stats.skew(residuals)
                residuals_kurt = stats.kurtosis(residuals)
                
                # Teste de normalidade
                try:
                    if len(residuals) <= 5000:
                        shapiro_stat, shapiro_p = stats.shapiro(residuals)
                    else:
                        shapiro_stat, shapiro_p = np.nan, np.nan
                except:
                    shapiro_stat, shapiro_p = np.nan, np.nan
                
                model_metrics[model_name] = {
                    'r2': r2, 'rmse': rmse, 'mae': mae, 'mape': mape,
                    'residuals_mean': residuals_mean, 'residuals_std': residuals_std,
                    'residuals_skew': residuals_skew, 'residuals_kurt': residuals_kurt,
                    'shapiro_stat': shapiro_stat, 'shapiro_p': shapiro_p,
                    'n_samples': len(actuals)
                }
        
        if report_type == "comparison" and len(model_metrics) > 1:
            # Ranking geral
            rankings = []
            for name, metrics in model_metrics.items():
                score = 0
                # R¬≤ (peso 3)
                score += metrics['r2'] * 3
                # RMSE invertido (peso 2)
                max_rmse = max([m['rmse'] for m in model_metrics.values()])
                score += (1 - metrics['rmse'] / max_rmse) * 2 if max_rmse > 0 else 0
                # MAE invertido (peso 2)
                max_mae = max([m['mae'] for m in model_metrics.values()])
                score += (1 - metrics['mae'] / max_mae) * 2 if max_mae > 0 else 0
                
                rankings.append((name, score, metrics))
            
            rankings.sort(key=lambda x: x[1], reverse=True)
            
            f.write("üèÜ RANKING GERAL DOS MODELOS:\n\n")
            for rank, (name, score, metrics) in enumerate(rankings, 1):
                medal = "ü•á" if rank == 1 else "ü•à" if rank == 2 else "ü•â" if rank == 3 else f"{rank}¬∫"
                f.write(f"{medal} {name}\n")
                f.write(f"    Score total: {score:.4f}\n")
                f.write(f"    R¬≤: {metrics['r2']:.6f}\n")
                f.write(f"    RMSE: {metrics['rmse']:.6f}\n")
                f.write(f"    MAE: {metrics['mae']:.6f}\n")
                if not np.isnan(metrics['mape']):
                    f.write(f"    MAPE: {metrics['mape']:.2f}%\n")
                f.write("\n")
            
            # An√°lise do melhor modelo
            best_name, best_score, best_metrics = rankings[0]
            f.write(f"üéØ AN√ÅLISE DO MELHOR MODELO ({best_name}):\n")
            if best_metrics['r2'] > 0.95:
                f.write("   ‚úÖ Performance EXCEPCIONAL (R¬≤ > 0.95)\n")
            elif best_metrics['r2'] > 0.9:
                f.write("   ‚úÖ Performance EXCELENTE (R¬≤ > 0.9)\n")
            elif best_metrics['r2'] > 0.8:
                f.write("   ‚úÖ Performance BOA (R¬≤ > 0.8)\n")
            elif best_metrics['r2'] > 0.6:
                f.write("   ‚ö†Ô∏è Performance MODERADA (R¬≤ > 0.6)\n")
            else:
                f.write("   ‚ùå Performance BAIXA (R¬≤ ‚â§ 0.6)\n")
            
            # An√°lise de res√≠duos do melhor modelo
            if abs(best_metrics['residuals_mean']) < best_metrics['residuals_std'] * 0.1:
                f.write("   ‚úÖ Res√≠duos bem centrados (sem vi√©s)\n")
            else:
                f.write("   ‚ö†Ô∏è Res√≠duos com poss√≠vel vi√©s\n")
            
            if abs(best_metrics['residuals_skew']) < 0.5:
                f.write("   ‚úÖ Res√≠duos aproximadamente sim√©tricos\n")
            else:
                f.write("   ‚ö†Ô∏è Res√≠duos com assimetria significativa\n")
        
        f.write("\n")
        
        # ========== AN√ÅLISE DETALHADA POR MODELO ==========
        f.write("AN√ÅLISE DETALHADA POR MODELO\n")
        f.write("-" * 50 + "\n\n")
        
        for model_name, metrics in model_metrics.items():
            f.write(f"üìä MODELO: {model_name}\n")
            f.write("=" * 40 + "\n")
            
            # Informa√ß√µes b√°sicas
            f.write(f"Tamanho da amostra: {metrics['n_samples']:,} pontos\n\n")
            
            # M√©tricas de performance
            f.write("üéØ M√âTRICAS DE PERFORMANCE:\n")
            f.write(f"   R¬≤ (Coeficiente de Determina√ß√£o): {metrics['r2']:.6f}\n")
            f.write(f"   RMSE (Raiz do Erro Quadr√°tico M√©dio): {metrics['rmse']:.6f}\n")
            f.write(f"   MAE (Erro Absoluto M√©dio): {metrics['mae']:.6f}\n")
            if not np.isnan(metrics['mape']):
                f.write(f"   MAPE (Erro Percentual Absoluto M√©dio): {metrics['mape']:.2f}%\n")
            f.write("\n")
            
            # Interpreta√ß√£o das m√©tricas
            f.write("üìà INTERPRETA√á√ÉO:\n")
            if metrics['r2'] > 0.9:
                f.write("   ‚Ä¢ Excelente capacidade preditiva\n")
            elif metrics['r2'] > 0.8:
                f.write("   ‚Ä¢ Boa capacidade preditiva\n")
            elif metrics['r2'] > 0.6:
                f.write("   ‚Ä¢ Capacidade preditiva moderada\n")
            else:
                f.write("   ‚Ä¢ Capacidade preditiva baixa\n")
            
            if not np.isnan(metrics['mape']):
                if metrics['mape'] < 5:
                    f.write("   ‚Ä¢ Erro percentual muito baixo (< 5%)\n")
                elif metrics['mape'] < 10:
                    f.write("   ‚Ä¢ Erro percentual baixo (< 10%)\n")
                elif metrics['mape'] < 20:
                    f.write("   ‚Ä¢ Erro percentual moderado (< 20%)\n")
                else:
                    f.write("   ‚Ä¢ Erro percentual alto (‚â• 20%)\n")
            
            f.write("\n")
            
            # An√°lise de res√≠duos
            f.write("üî¨ AN√ÅLISE DE RES√çDUOS:\n")
            f.write(f"   M√©dia: {metrics['residuals_mean']:.6f}\n")
            f.write(f"   Desvio padr√£o: {metrics['residuals_std']:.6f}\n")
            f.write(f"   Assimetria: {metrics['residuals_skew']:.4f}\n")
            f.write(f"   Curtose: {metrics['residuals_kurt']:.4f}\n")
            
            if not np.isnan(metrics['shapiro_stat']):
                f.write(f"   Teste Shapiro-Wilk: {metrics['shapiro_stat']:.4f} (p-valor: {metrics['shapiro_p']:.4f})\n")
                if metrics['shapiro_p'] > 0.05:
                    f.write("   ‚úÖ Res√≠duos seguem distribui√ß√£o normal (p > 0.05)\n")
                else:
                    f.write("   ‚ö†Ô∏è Res√≠duos n√£o seguem distribui√ß√£o normal (p ‚â§ 0.05)\n")
            
            # Diagn√≥stico dos res√≠duos
            f.write("\nü©∫ DIAGN√ìSTICO DOS RES√çDUOS:\n")
            
            # Vi√©s
            if abs(metrics['residuals_mean']) < metrics['residuals_std'] * 0.1:
                f.write("   ‚úÖ Sem vi√©s significativo\n")
            else:
                f.write("   ‚ö†Ô∏è Poss√≠vel vi√©s nos res√≠duos\n")
            
            # Simetria
            if abs(metrics['residuals_skew']) < 0.5:
                f.write("   ‚úÖ Distribui√ß√£o aproximadamente sim√©trica\n")
            elif abs(metrics['residuals_skew']) < 1:
                f.write("   ‚ö†Ô∏è Assimetria leve\n")
            else:
                f.write("   ‚ùå Assimetria significativa\n")
            
            # Curtose
            if abs(metrics['residuals_kurt']) < 1:
                f.write("   ‚úÖ Curtose normal\n")
            elif abs(metrics['residuals_kurt']) < 2:
                f.write("   ‚ö†Ô∏è Curtose moderada\n")
            else:
                f.write("   ‚ùå Curtose excessiva\n")
            
            f.write("\n" + "‚îÄ" * 40 + "\n\n")
        
        # ========== RECOMENDA√á√ïES ==========
        f.write("RECOMENDA√á√ïES E CONCLUS√ïES\n")
        f.write("-" * 50 + "\n\n")
        
        if report_type == "comparison" and len(model_metrics) > 1:
            best_name = rankings[0][0]
            f.write(f"üéØ MODELO RECOMENDADO: {best_name}\n\n")
            
            f.write("üí° JUSTIFICATIVAS:\n")
            best_metrics = model_metrics[best_name]
            
            if best_metrics['r2'] > 0.8:
                f.write(f"   ‚Ä¢ Alto R¬≤ ({best_metrics['r2']:.4f}) indica boa capacidade preditiva\n")
            
            if best_metrics['rmse'] == min([m['rmse'] for m in model_metrics.values()]):
                f.write("   ‚Ä¢ Menor RMSE entre todos os modelos\n")
            
            if abs(best_metrics['residuals_mean']) < best_metrics['residuals_std'] * 0.1:
                f.write("   ‚Ä¢ Res√≠duos bem centrados (sem vi√©s)\n")
            
            f.write("\nüöÄ PR√ìXIMOS PASSOS:\n")
            if best_metrics['r2'] < 0.9:
                f.write("   ‚Ä¢ Considerar ajustes nos hiperpar√¢metros\n")
                f.write("   ‚Ä¢ Avaliar engenharia de features\n")
            
            if abs(best_metrics['residuals_skew']) > 0.5:
                f.write("   ‚Ä¢ Investigar outliers nos dados\n")
                f.write("   ‚Ä¢ Considerar transforma√ß√µes nos dados\n")
            
            if not np.isnan(best_metrics['shapiro_p']) and best_metrics['shapiro_p'] <= 0.05:
                f.write("   ‚Ä¢ Verificar premissas do modelo\n")
                f.write("   ‚Ä¢ Considerar modelos n√£o-param√©tricos\n")
        else:
            # An√°lise de modelo √∫nico
            model_name = list(model_metrics.keys())[0]
            metrics = model_metrics[model_name]
            
            f.write(f"üìä AVALIA√á√ÉO DO MODELO: {model_name}\n\n")
            
            if metrics['r2'] > 0.9:
                f.write("‚úÖ VEREDICTO: Modelo EXCELENTE para uso em produ√ß√£o\n")
            elif metrics['r2'] > 0.8:
                f.write("‚úÖ VEREDICTO: Modelo BOM, adequado para uso\n")
            elif metrics['r2'] > 0.6:
                f.write("‚ö†Ô∏è VEREDICTO: Modelo MODERADO, necessita melhorias\n")
            else:
                f.write("‚ùå VEREDICTO: Modelo INADEQUADO, requer revis√£o completa\n")
            
            f.write("\nüéØ PONTOS FORTES:\n")
            if metrics['r2'] > 0.8:
                f.write("   ‚Ä¢ Boa capacidade de explica√ß√£o da variabilidade\n")
            if abs(metrics['residuals_mean']) < metrics['residuals_std'] * 0.1:
                f.write("   ‚Ä¢ Res√≠duos sem vi√©s significativo\n")
            if abs(metrics['residuals_skew']) < 0.5:
                f.write("   ‚Ä¢ Distribui√ß√£o de res√≠duos sim√©trica\n")
            
            f.write("\n‚ö†Ô∏è PONTOS DE ATEN√á√ÉO:\n")
            if metrics['r2'] < 0.8:
                f.write("   ‚Ä¢ R¬≤ relativamente baixo\n")
            if abs(metrics['residuals_mean']) >= metrics['residuals_std'] * 0.1:
                f.write("   ‚Ä¢ Poss√≠vel vi√©s nos res√≠duos\n")
            if abs(metrics['residuals_skew']) >= 0.5:
                f.write("   ‚Ä¢ Assimetria nos res√≠duos\n")
        
        f.write("\n")
        f.write("=" * 80 + "\n")
        f.write("FIM DO RELAT√ìRIO\n")
        f.write("=" * 80 + "\n")
    
    print_save_message(save_path, "Relat√≥rio textual")


def generate_html_report(results_dict, generated_files, save_path, report_type="comparison"):
    """
    Gera relat√≥rio HTML interativo
    
    Args:
        results_dict: Dicion√°rio com resultados dos modelos
        generated_files: Dicion√°rio com caminhos dos arquivos gerados
        save_path: Caminho para salvar o relat√≥rio HTML
        report_type: Tipo do relat√≥rio ("single" ou "comparison")
    """
    clean_results = validate_and_clean_metrics(results_dict)
    
    html_content = f"""
    <!DOCTYPE html>
    <html lang="pt-BR">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Relat√≥rio de An√°lise de Modelos</title>
        <style>
            body {{
                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 20px;
                background-color: #f5f5f5;
            }}
            .container {{
                max-width: 1200px;
                margin: 0 auto;
                background-color: white;
                padding: 30px;
                border-radius: 10px;
                box-shadow: 0 0 20px rgba(0,0,0,0.1);
            }}
            h1 {{
                color: #2c3e50;
                text-align: center;
                border-bottom: 3px solid #3498db;
                padding-bottom: 10px;
            }}
            h2 {{
                color: #34495e;
                margin-top: 30px;
                border-left: 4px solid #3498db;
                padding-left: 15px;
            }}
            h3 {{
                color: #555;
                margin-top: 25px;
            }}
            .summary-box {{
                background-color: #ecf0f1;
                padding: 20px;
                border-radius: 8px;
                margin: 20px 0;
                border-left: 5px solid #3498db;
            }}
            .metric-card {{
                background-color: #fff;
                border: 1px solid #ddd;
                border-radius: 8px;
                padding: 15px;
                margin: 10px 0;
                box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            }}
            .metric-value {{
                font-size: 1.2em;
                font-weight: bold;
                color: #2980b9;
            }}
            .image-gallery {{
                display: grid;
                grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
                gap: 20px;
                margin: 20px 0;
            }}
            .image-card {{
                text-align: center;
                background-color: #fff;
                border-radius: 8px;
                padding: 15px;
                box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            }}
            .image-card img {{
                max-width: 100%;
                height: auto;
                border-radius: 5px;
                border: 1px solid #ddd;
            }}
            .image-card h4 {{
                margin: 10px 0 5px 0;
                color: #2c3e50;
            }}
            .timestamp {{
                text-align: center;
                color: #7f8c8d;
                font-style: italic;
                margin-bottom: 30px;
            }}
            .navigation {{
                background-color: #34495e;
                padding: 10px;
                border-radius: 5px;
                margin-bottom: 20px;
            }}
            .navigation a {{
                color: white;
                text-decoration: none;
                margin: 0 15px;
                padding: 5px 10px;
                border-radius: 3px;
                transition: background-color 0.3s;
            }}
            .navigation a:hover {{
                background-color: #2c3e50;
            }}
            .model-section {{
                border: 1px solid #ddd;
                border-radius: 8px;
                margin: 15px 0;
                padding: 20px;
                background-color: #fafafa;
            }}
            .status-good {{ color: #27ae60; }}
            .status-warning {{ color: #f39c12; }}
            .status-bad {{ color: #e74c3c; }}
        </style>
    </head>
    <body>
        <div class="container">
            <h1>üìä Relat√≥rio de An√°lise de Modelos</h1>
            <div class="timestamp">
                Gerado em: {datetime.now().strftime('%d/%m/%Y √†s %H:%M:%S')}
            </div>
            
            <div class="navigation">
                <a href="#resumo">üìã Resumo</a>
                <a href="#metricas">üìà M√©tricas</a>
                <a href="#graficos">üìä Gr√°ficos</a>
                <a href="#analises">üî¨ An√°lises</a>
                <a href="#arquivos">üìÅ Arquivos</a>
            </div>
    """
    
    # ========== RESUMO ==========
    html_content += f"""
            <div id="resumo">
                <h2>üìã Resumo Executivo</h2>
                <div class="summary-box">
                    <p><strong>Tipo de an√°lise:</strong> {'Compara√ß√£o de m√∫ltiplos modelos' if report_type == 'comparison' else 'An√°lise de modelo √∫nico'}</p>
                    <p><strong>N√∫mero de modelos:</strong> {len(clean_results)}</p>
                    <p><strong>Modelos analisados:</strong> {', '.join(clean_results.keys())}</p>
                </div>
            </div>
    """
    
    # ========== M√âTRICAS ==========
    html_content += """
            <div id="metricas">
                <h2>üìà M√©tricas de Performance</h2>
    """
    
    from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
    
    for model_name, results in clean_results.items():
        if 'actuals' in results and 'predictions' in results:
            actuals = np.array(results['actuals']).flatten()
            predictions = np.array(results['predictions']).flatten()
            
            r2 = r2_score(actuals, predictions)
            rmse = np.sqrt(mean_squared_error(actuals, predictions))
            mae = mean_absolute_error(actuals, predictions)
            mape = np.mean(np.abs((actuals - predictions) / actuals)) * 100 if np.all(actuals != 0) else np.nan
            
            # Determinar status
            if r2 > 0.9:
                status_class = "status-good"
                status_text = "Excelente"
            elif r2 > 0.8:
                status_class = "status-good"
                status_text = "Bom"
            elif r2 > 0.6:
                status_class = "status-warning"
                status_text = "Moderado"
            else:
                status_class = "status-bad"
                status_text = "Baixo"
            
            html_content += f"""
                <div class="model-section">
                    <h3>{model_name} <span class="{status_class}">({status_text})</span></h3>
                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px;">
                        <div class="metric-card">
                            <h4>R¬≤ (Coeficiente de Determina√ß√£o)</h4>
                            <div class="metric-value">{r2:.6f}</div>
                        </div>
                        <div class="metric-card">
                            <h4>RMSE</h4>
                            <div class="metric-value">{rmse:.6f}</div>
                        </div>
                        <div class="metric-card">
                            <h4>MAE</h4>
                            <div class="metric-value">{mae:.6f}</div>
                        </div>
            """
            
            if not np.isnan(mape):
                html_content += f"""
                        <div class="metric-card">
                            <h4>MAPE (%)</h4>
                            <div class="metric-value">{mape:.2f}%</div>
                        </div>
                """
            
            html_content += """
                    </div>
                </div>
            """
    
    html_content += "</div>"
    
    # ========== GR√ÅFICOS ==========
    html_content += """
            <div id="graficos">
                <h2>üìä Visualiza√ß√µes</h2>
                <div class="image-gallery">
    """
    
    for file_key, file_path in generated_files.items():
        if file_path.endswith('.png'):
            file_name = os.path.basename(file_path)
            # Criar t√≠tulo mais amig√°vel
            if 'overview' in file_key:
                title = "Vis√£o Geral Comparativa"
            elif 'training' in file_key:
                title = f"Hist√≥rico de Treinamento"
            elif 'predictions' in file_key:
                title = f"Predi√ß√µes vs Valores Reais"
            elif 'qq' in file_key:
                title = f"An√°lise Q-Q Plot"
            elif 'cdf' in file_key:
                title = f"Compara√ß√£o de CDF"
            elif 'pdf' in file_key:
                title = f"Compara√ß√£o de FDP"
            elif 'ks' in file_key:
                title = f"Teste Kolmogorov-Smirnov"
            elif 'radar' in file_key:
                title = "Gr√°fico Radar de Performance"
            elif 'metrics' in file_key:
                title = "Tabela de M√©tricas"
            else:
                title = file_name.replace('_', ' ').replace('.png', '').title()
            
            html_content += f"""
                <div class="image-card">
                    <h4>{title}</h4>
                    <img src="{file_name}" alt="{title}">
                </div>
            """
    
    html_content += """
                </div>
            </div>
    """
    
    # ========== AN√ÅLISES ==========
    html_content += """
            <div id="analises">
                <h2>üî¨ An√°lises Estat√≠sticas</h2>
                <div class="summary-box">
                    <p>As an√°lises estat√≠sticas detalhadas incluem:</p>
                    <ul>
                        <li><strong>Q-Q Plot:</strong> Compara√ß√£o das distribui√ß√µes dos res√≠duos com a distribui√ß√£o normal</li>
                        <li><strong>CDF/FDP:</strong> An√°lise das fun√ß√µes de distribui√ß√£o emp√≠ricas</li>
                        <li><strong>Teste KS:</strong> Teste de Kolmogorov-Smirnov para compara√ß√£o de distribui√ß√µes</li>
                        <li><strong>Autocorrela√ß√£o:</strong> An√°lise da depend√™ncia temporal (quando aplic√°vel)</li>
                    </ul>
                    <p>Para mais detalhes, consulte o <a href="relatorio_textual_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt">relat√≥rio textual</a>.</p>
                </div>
            </div>
    """
    
    # ========== ARQUIVOS ==========
    html_content += """
            <div id="arquivos">
                <h2>üìÅ Arquivos Gerados</h2>
                <div class="summary-box">
                    <h3>Lista de Arquivos:</h3>
                    <ul>
    """
    
    for file_key, file_path in generated_files.items():
        file_name = os.path.basename(file_path)
        file_type = "Imagem" if file_path.endswith('.png') else "Texto" if file_path.endswith('.txt') else "HTML"
        html_content += f"""
                        <li><strong>{file_type}:</strong> <a href="{file_name}">{file_name}</a></li>
        """
    
    html_content += """
                    </ul>
                </div>
            </div>
        </div>
    </body>
    </html>
    """
    
    with open(save_path, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    print_save_message(save_path, "Relat√≥rio HTML")


def generate_quick_summary(results_dict, model_name=None):
    """
    Gera resumo r√°pido para visualiza√ß√£o no console
    
    Args:
        results_dict: Dicion√°rio com resultados dos modelos
        model_name: Nome do modelo (para an√°lise de modelo √∫nico)
    
    Returns:
        str: Resumo formatado
    """
    from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
    
    if model_name and isinstance(results_dict, dict) and 'actuals' in results_dict:
        # An√°lise de um √∫nico modelo
        single_model_results = {model_name: results_dict}
    else:
        # M√∫ltiplos modelos
        single_model_results = results_dict
    
    clean_results = validate_and_clean_metrics(single_model_results)
    
    if not clean_results:
        return "‚ùå Dados insuficientes para gerar resumo."
    
    summary = "\n" + "="*60 + "\n"
    summary += "üìä RESUMO R√ÅPIDO DE PERFORMANCE\n"
    summary += "="*60 + "\n"
    
    model_scores = []
    
    for model_name, results in clean_results.items():
        if 'actuals' in results and 'predictions' in results:
            actuals = np.array(results['actuals']).flatten()
            predictions = np.array(results['predictions']).flatten()
            
            r2 = r2_score(actuals, predictions)
            rmse = np.sqrt(mean_squared_error(actuals, predictions))
            mae = mean_absolute_error(actuals, predictions)
            
            # Score simples para ranking
            score = r2 * 100 - (rmse + mae) * 10
            model_scores.append((model_name, score, r2, rmse, mae))
            
            # Status baseado em R¬≤
            if r2 > 0.9:
                status = "üü¢ EXCELENTE"
            elif r2 > 0.8:
                status = "üü¢ BOM"
            elif r2 > 0.6:
                status = "üü° MODERADO"
            else:
                status = "üî¥ BAIXO"
            
            summary += f"\nüìà {model_name}:\n"
            summary += f"   Status: {status}\n"
            summary += f"   R¬≤: {r2:.6f} | RMSE: {rmse:.6f} | MAE: {mae:.6f}\n"
    
    # Ranking se houver m√∫ltiplos modelos
    if len(model_scores) > 1:
        model_scores.sort(key=lambda x: x[1], reverse=True)
        summary += "\nüèÜ RANKING:\n"
        for rank, (name, score, r2, rmse, mae) in enumerate(model_scores, 1):
            medal = "ü•á" if rank == 1 else "ü•à" if rank == 2 else "ü•â" if rank == 3 else f"{rank}¬∫"
            summary += f"   {medal} {name} (Score: {score:.2f})\n"
        
        # Recomenda√ß√£o
        best_model = model_scores[0][0]
        summary += f"\nüéØ RECOMENDADO: {best_model}\n"
    
    summary += "\n" + "="*60 + "\n"
    
    return summary 