"""
Gera√ß√£o de relat√≥rios abrangentes com m√∫ltiplas visualiza√ß√µes
"""
import matplotlib
matplotlib.use('Agg')  # Backend n√£o-interativo que apenas salva arquivos
import matplotlib.pyplot as plt
plt.ioff()  # Desabilitar modo interativo

import os
from datetime import datetime
import numpy as np
import pandas as pd
from io import StringIO

from .utils import ensure_output_dir, print_save_message, validate_and_clean_metrics, format_metric_value, get_medal_emoji, get_status_emoji
from .basic_plots import plot_training_history, plot_predictions, plot_metrics_comparison, save_metrics_table
from .distribution_analysis import plot_qq_analysis, plot_cdf_comparison, plot_pdf_comparison
from .statistical_tests import plot_ks_test_analysis, plot_autocorrelation_analysis
from .comparison_plots import plot_models_comparison_overview
from .interactive_html import generate_interactive_html_report


def generate_comprehensive_report(results_dict, output_dir, model_name=None):
    """
    Gera relat√≥rio abrangente com todas as an√°lises dispon√≠veis
    
    Args:
        results_dict: Dicion√°rio com resultados dos modelos (ou resultados de um √∫nico modelo)
        output_dir: Diret√≥rio para salvar o relat√≥rio
        model_name: Nome do modelo (usado quando results_dict cont√©m dados de um √∫nico modelo)
    
    Returns:
        dict: Dicion√°rio com caminhos dos arquivos gerados
    """
    ensure_output_dir(output_dir)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    generated_files = {}
    
    # Determinar se √© an√°lise de um modelo ou compara√ß√£o
    if model_name and isinstance(results_dict, dict) and 'actuals' in results_dict:
        # An√°lise de um √∫nico modelo
        single_model_results = {model_name: results_dict}
        report_type = "single"
        report_title = f"Relat√≥rio Abrangente - {model_name}"
    else:
        # Compara√ß√£o de m√∫ltiplos modelos
        single_model_results = results_dict
        report_type = "comparison"
        report_title = "Relat√≥rio Comparativo de Modelos"
    
    print(f"\nüîÑ Gerando relat√≥rio abrangente ({report_type})...")
    print(f"üìÅ Diret√≥rio de sa√≠da: {output_dir}")
    
    # ========== AN√ÅLISE GERAL ==========
    if report_type == "comparison":
        print("\nüìä Gerando vis√£o geral comparativa...")
        overview_path = os.path.join(output_dir, f"01_visao_geral_{timestamp}.png")
        try:
            plot_models_comparison_overview(single_model_results, save_path=overview_path)
            generated_files['overview'] = overview_path
        except Exception as e:
            print(f"‚ö†Ô∏è Erro na vis√£o geral: {e}")
    
    # ========== GR√ÅFICOS B√ÅSICOS ==========
    print("\nüìà Gerando gr√°ficos b√°sicos...")
    
    # Para cada modelo individualmente
    for i, (model_name, results) in enumerate(single_model_results.items(), 1):
        model_prefix = f"{i:02d}_{model_name.replace(' ', '_')}"
        
        # Hist√≥rico de treinamento
        if 'train_losses' in results and 'val_losses' in results:
            training_path = os.path.join(output_dir, f"{model_prefix}_treinamento_{timestamp}.png")
            try:
                plot_training_history(results['train_losses'], results['val_losses'], 
                                    save_path=training_path, title=f"Hist√≥rico de Treinamento - {model_name}")
                generated_files[f'training_{model_name}'] = training_path
            except Exception as e:
                print(f"‚ö†Ô∏è Erro no hist√≥rico de treinamento para {model_name}: {e}")
        
        # Predi√ß√µes vs Reais
        if 'actuals' in results and 'predictions' in results:
            predictions_path = os.path.join(output_dir, f"{model_prefix}_predicoes_{timestamp}.png")
            try:
                plot_predictions(results['actuals'], results['predictions'], 
                               save_path=predictions_path, title=f"Predi√ß√µes vs Valores Reais - {model_name}")
                generated_files[f'predictions_{model_name}'] = predictions_path
            except Exception as e:
                print(f"‚ö†Ô∏è Erro nas predi√ß√µes para {model_name}: {e}")
    
    # ========== AN√ÅLISES ESTAT√çSTICAS ==========
    print("\nüî¨ Gerando an√°lises estat√≠sticas...")
    
    for i, (model_name, results) in enumerate(single_model_results.items(), 1):
        if 'actuals' not in results or 'predictions' not in results:
            continue
            
        model_prefix = f"{i:02d}_{model_name.replace(' ', '_')}"
        
        # QQ-Plot
        qq_path = os.path.join(output_dir, f"{model_prefix}_qq_plot_{timestamp}.png")
        try:
            plot_qq_analysis(results['actuals'], results['predictions'], 
                           save_path=qq_path, title=f"QQ-Plot - {model_name}")
            generated_files[f'qq_{model_name}'] = qq_path
        except Exception as e:
            print(f"‚ö†Ô∏è Erro no Q-Q plot para {model_name}: {e}")
        
        # An√°lise de distribui√ß√µes (CDF e PDF)
        cdf_path = os.path.join(output_dir, f"{model_prefix}_cdf_{timestamp}.png")
        try:
            plot_cdf_comparison(results['actuals'], results['predictions'], 
                              save_path=cdf_path, title=f"Compara√ß√£o FDA - {model_name}")
            generated_files[f'cdf_{model_name}'] = cdf_path
        except Exception as e:
            print(f"‚ö†Ô∏è Erro na an√°lise CDF para {model_name}: {e}")
        
        pdf_path = os.path.join(output_dir, f"{model_prefix}_pdf_{timestamp}.png")
        try:
            plot_pdf_comparison(results['actuals'], results['predictions'], 
                              save_path=pdf_path, title=f"Compara√ß√£o FDP - {model_name}")
            generated_files[f'pdf_{model_name}'] = pdf_path
        except Exception as e:
            print(f"‚ö†Ô∏è Erro na an√°lise FDP para {model_name}: {e}")
        
        # Teste de Kolmogorov-Smirnov
        ks_path = os.path.join(output_dir, f"{model_prefix}_ks_test_{timestamp}.png")
        try:
            plot_ks_test_analysis(results['actuals'], results['predictions'], 
                                 save_path=ks_path, title=f"Teste Kolmogorov-Smirnov - {model_name}")
            generated_files[f'ks_{model_name}'] = ks_path
        except Exception as e:
            print(f"‚ö†Ô∏è Erro no teste KS para {model_name}: {e}")
        
        # An√°lise de autocorrela√ß√£o (se existir s√©rie temporal)
        if 'actuals' in results and 'predictions' in results:
            autocorr_path = os.path.join(output_dir, f"{model_prefix}_autocorrelacao_{timestamp}.png")
            try:
                plot_autocorrelation_analysis(results['actuals'], results['predictions'], 
                                             save_path=autocorr_path, title=f"An√°lise de Autocorrela√ß√£o - {model_name}")
                generated_files[f'autocorr_{model_name}'] = autocorr_path
            except Exception as e:
                print(f"‚ö†Ô∏è Erro na an√°lise de autocorrela√ß√£o para {model_name}: {e}")
    
    # ========== AN√ÅLISES COMPARATIVAS ==========
    if report_type == "comparison" and len(single_model_results) > 1:
        print("\nüÜö Gerando an√°lises comparativas...")
        
        # Tabela de m√©tricas
        metrics_table_path = os.path.join(output_dir, f"99_tabela_metricas_{timestamp}.png")
        metrics_table_comparison = os.path.join(output_dir, f"99_comparacao_metricas_{timestamp}.png")
        try:
            plot_metrics_comparison(single_model_results, save_path=metrics_table_comparison)
            save_metrics_table(single_model_results, metrics_table_path)
            generated_files['metrics_table'] = metrics_table_path
            generated_files['metrics_comparison'] = metrics_table_comparison
        except Exception as e:
            print(f"‚ö†Ô∏è Erro na tabela de m√©tricas: {e}")
    
    # ========== RELAT√ìRIO TEXTUAL ==========
    print("\nüìÑ Gerando relat√≥rio textual...")
    text_report_path = os.path.join(output_dir, f"relatorio_textual_{timestamp}.txt")
    try:
        generate_text_report(single_model_results, text_report_path, report_type)
        generated_files['text_report'] = text_report_path
    except Exception as e:
        print(f"‚ö†Ô∏è Erro no relat√≥rio textual: {e}")
    
    # ========== RELAT√ìRIO HTML INTERATIVO ==========
    print("\nüåê Gerando relat√≥rio HTML interativo...")
    interactive_html_path = os.path.join(output_dir, f"relatorio_interativo_{timestamp}.html")
    try:
        generate_interactive_html_report(single_model_results, generated_files, interactive_html_path, report_type)
        generated_files['interactive_html_report'] = interactive_html_path
    except Exception as e:
        print(f"‚ö†Ô∏è Erro no relat√≥rio HTML interativo: {e}")
    
    print(f"\n‚úÖ Relat√≥rio abrangente gerado com sucesso!")
    print(f"üìÅ {len(generated_files)} arquivos gerados em: {output_dir}")
    
    return generated_files


def generate_text_report(results_dict, save_path, report_type="comparison"):
    """
    Gera relat√≥rio textual detalhado
    
    Args:
        results_dict: Dicion√°rio com resultados dos modelos
        save_path: Caminho para salvar o relat√≥rio
        report_type: Tipo do relat√≥rio ("single" ou "comparison")
    """
    from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
    from scipy import stats
    
    clean_results = validate_and_clean_metrics(results_dict)
    
    with open(save_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("RELAT√ìRIO DETALHADO DE AN√ÅLISE DE MODELOS\n")
        f.write("=" * 80 + "\n")
        f.write(f"Gerado em: {datetime.now().strftime('%d/%m/%Y √†s %H:%M:%S')}\n")
        f.write(f"Tipo de an√°lise: {'Compara√ß√£o de m√∫ltiplos modelos' if report_type == 'comparison' else 'An√°lise de modelo √∫nico'}\n")
        f.write(f"N√∫mero de modelos analisados: {len(clean_results)}\n\n")
        
        # ========== RESUMO EXECUTIVO ==========
        f.write("RESUMO EXECUTIVO\n")
        f.write("-" * 50 + "\n\n")
        
        if not clean_results:
            f.write("‚ùå ERRO: Dados insuficientes para an√°lise.\n")
            return
        
        # Calcular m√©tricas para todos os modelos
        model_metrics = {}
        for model_name, results in clean_results.items():
            if 'actuals' in results and 'predictions' in results:
                actuals = np.array(results['actuals']).flatten()
                predictions = np.array(results['predictions']).flatten()
                
                # M√©tricas de performance
                r2 = r2_score(actuals, predictions)
                rmse = np.sqrt(mean_squared_error(actuals, predictions))
                mae = mean_absolute_error(actuals, predictions)
                mape = np.mean(np.abs((actuals - predictions) / actuals)) * 100 if np.all(actuals != 0) else np.nan
                
                # An√°lise de res√≠duos
                residuals = predictions - actuals
                residuals_mean = np.mean(residuals)
                residuals_std = np.std(residuals)
                residuals_skew = stats.skew(residuals)
                residuals_kurt = stats.kurtosis(residuals)
                
                # Teste de normalidade
                try:
                    if len(residuals) <= 5000:
                        shapiro_stat, shapiro_p = stats.shapiro(residuals)
                    else:
                        shapiro_stat, shapiro_p = np.nan, np.nan
                except:
                    shapiro_stat, shapiro_p = np.nan, np.nan
                
                model_metrics[model_name] = {
                    'r2': r2, 'rmse': rmse, 'mae': mae, 'mape': mape,
                    'residuals_mean': residuals_mean, 'residuals_std': residuals_std,
                    'residuals_skew': residuals_skew, 'residuals_kurt': residuals_kurt,
                    'shapiro_stat': shapiro_stat, 'shapiro_p': shapiro_p,
                    'n_samples': len(actuals)
                }
        
        if report_type == "comparison" and len(model_metrics) > 1:
            # Ranking geral
            rankings = []
            for name, metrics in model_metrics.items():
                score = 0
                # R¬≤ (peso 3)
                score += metrics['r2'] * 3
                # RMSE invertido (peso 2)
                max_rmse = max([m['rmse'] for m in model_metrics.values()])
                score += (1 - metrics['rmse'] / max_rmse) * 2 if max_rmse > 0 else 0
                # MAE invertido (peso 2)
                max_mae = max([m['mae'] for m in model_metrics.values()])
                score += (1 - metrics['mae'] / max_mae) * 2 if max_mae > 0 else 0
                
                rankings.append((name, score, metrics))
            
            rankings.sort(key=lambda x: x[1], reverse=True)
            
            f.write("üèÜ RANKING GERAL DOS MODELOS:\n\n")
            for rank, (name, score, metrics) in enumerate(rankings, 1):
                medal = get_medal_emoji(rank)
                f.write(f"{medal} {name}\n")
                f.write(f"    Score total: {score:.4f}\n")
                f.write(f"    R¬≤: {metrics['r2']:.6f}\n")
                f.write(f"    RMSE: {metrics['rmse']:.6f}\n")
                f.write(f"    MAE: {metrics['mae']:.6f}\n")
                if not np.isnan(metrics['mape']):
                    f.write(f"    MAPE: {metrics['mape']:.2f}%\n")
                f.write("\n")
            
            # An√°lise do melhor modelo
            best_name, best_score, best_metrics = rankings[0]
            f.write(f"üéØ AN√ÅLISE DO MELHOR MODELO ({best_name}):\n")
            if best_metrics['r2'] > 0.95:
                f.write("   ‚úÖ Performance EXCEPCIONAL (R¬≤ > 0.95)\n")
            elif best_metrics['r2'] > 0.9:
                f.write("   ‚úÖ Performance EXCELENTE (R¬≤ > 0.9)\n")
            elif best_metrics['r2'] > 0.8:
                f.write("   ‚úÖ Performance BOA (R¬≤ > 0.8)\n")
            elif best_metrics['r2'] > 0.6:
                f.write("   ‚ö†Ô∏è Performance MODERADA (R¬≤ > 0.6)\n")
            else:
                f.write("   ‚ùå Performance BAIXA (R¬≤ ‚â§ 0.6)\n")
            
            # An√°lise de res√≠duos do melhor modelo
            if abs(best_metrics['residuals_mean']) < best_metrics['residuals_std'] * 0.1:
                f.write("   ‚úÖ Res√≠duos bem centrados (sem vi√©s)\n")
            else:
                f.write("   ‚ö†Ô∏è Res√≠duos com poss√≠vel vi√©s\n")
            
            if abs(best_metrics['residuals_skew']) < 0.5:
                f.write("   ‚úÖ Res√≠duos aproximadamente sim√©tricos\n")
            else:
                f.write("   ‚ö†Ô∏è Res√≠duos com assimetria significativa\n")
        
        f.write("\n")
        
        # ========== AN√ÅLISE DETALHADA POR MODELO ==========
        f.write("AN√ÅLISE DETALHADA POR MODELO\n")
        f.write("-" * 50 + "\n\n")
        
        for model_name, metrics in model_metrics.items():
            f.write(f"üìä MODELO: {model_name}\n")
            f.write("=" * 40 + "\n")
            
            # Informa√ß√µes b√°sicas
            f.write(f"Tamanho da amostra: {metrics['n_samples']:,} pontos\n\n")
            
            # M√©tricas de performance
            f.write("üéØ M√âTRICAS DE PERFORMANCE:\n")
            f.write(f"   R¬≤ (Coeficiente de Determina√ß√£o): {metrics['r2']:.6f}\n")
            f.write(f"   RMSE (Raiz do Erro Quadr√°tico M√©dio): {metrics['rmse']:.6f}\n")
            f.write(f"   MAE (Erro Absoluto M√©dio): {metrics['mae']:.6f}\n")
            if not np.isnan(metrics['mape']):
                f.write(f"   MAPE (Erro Percentual Absoluto M√©dio): {metrics['mape']:.2f}%\n")
            f.write("\n")
            
            # Interpreta√ß√£o das m√©tricas
            f.write("üìà INTERPRETA√á√ÉO:\n")
            if metrics['r2'] > 0.9:
                f.write("   ‚Ä¢ Excelente capacidade preditiva\n")
            elif metrics['r2'] > 0.8:
                f.write("   ‚Ä¢ Boa capacidade preditiva\n")
            elif metrics['r2'] > 0.6:
                f.write("   ‚Ä¢ Capacidade preditiva moderada\n")
            else:
                f.write("   ‚Ä¢ Capacidade preditiva baixa\n")
            
            if not np.isnan(metrics['mape']):
                if metrics['mape'] < 5:
                    f.write("   ‚Ä¢ Erro percentual muito baixo (< 5%)\n")
                elif metrics['mape'] < 10:
                    f.write("   ‚Ä¢ Erro percentual baixo (< 10%)\n")
                elif metrics['mape'] < 20:
                    f.write("   ‚Ä¢ Erro percentual moderado (< 20%)\n")
                else:
                    f.write("   ‚Ä¢ Erro percentual alto (‚â• 20%)\n")
            
            f.write("\n")
            
            # An√°lise de res√≠duos
            f.write("üî¨ AN√ÅLISE DE RES√çDUOS:\n")
            f.write(f"   M√©dia: {metrics['residuals_mean']:.6f}\n")
            f.write(f"   Desvio padr√£o: {metrics['residuals_std']:.6f}\n")
            f.write(f"   Assimetria: {metrics['residuals_skew']:.4f}\n")
            f.write(f"   Curtose: {metrics['residuals_kurt']:.4f}\n")
            
            if not np.isnan(metrics['shapiro_stat']):
                f.write(f"   Teste Shapiro-Wilk: {metrics['shapiro_stat']:.4f} (p-valor: {metrics['shapiro_p']:.4f})\n")
                if metrics['shapiro_p'] > 0.05:
                    f.write("   ‚úÖ Res√≠duos seguem distribui√ß√£o normal (p > 0.05)\n")
                else:
                    f.write("   ‚ö†Ô∏è Res√≠duos n√£o seguem distribui√ß√£o normal (p ‚â§ 0.05)\n")
            
            # Diagn√≥stico dos res√≠duos
            f.write("\nü©∫ DIAGN√ìSTICO DOS RES√çDUOS:\n")
            
            # Vi√©s
            if abs(metrics['residuals_mean']) < metrics['residuals_std'] * 0.1:
                f.write("   ‚úÖ Sem vi√©s significativo\n")
            else:
                f.write("   ‚ö†Ô∏è Poss√≠vel vi√©s nos res√≠duos\n")
            
            # Simetria
            if abs(metrics['residuals_skew']) < 0.5:
                f.write("   ‚úÖ Distribui√ß√£o aproximadamente sim√©trica\n")
            elif abs(metrics['residuals_skew']) < 1:
                f.write("   ‚ö†Ô∏è Assimetria leve\n")
            else:
                f.write("   ‚ùå Assimetria significativa\n")
            
            # Curtose
            if abs(metrics['residuals_kurt']) < 1:
                f.write("   ‚úÖ Curtose normal\n")
            elif abs(metrics['residuals_kurt']) < 2:
                f.write("   ‚ö†Ô∏è Curtose moderada\n")
            else:
                f.write("   ‚ùå Curtose excessiva\n")
            
            f.write("\n" + "‚îÄ" * 40 + "\n\n")
        
        # ========== RECOMENDA√á√ïES ==========
        f.write("RECOMENDA√á√ïES E CONCLUS√ïES\n")
        f.write("-" * 50 + "\n\n")
        
        if report_type == "comparison" and len(model_metrics) > 1:
            best_name = rankings[0][0]
            f.write(f"üéØ MODELO RECOMENDADO: {best_name}\n\n")
            
            f.write("üí° JUSTIFICATIVAS:\n")
            best_metrics = model_metrics[best_name]
            
            if best_metrics['r2'] > 0.8:
                f.write(f"   ‚Ä¢ Alto R¬≤ ({best_metrics['r2']:.4f}) indica boa capacidade preditiva\n")
            
            if best_metrics['rmse'] == min([m['rmse'] for m in model_metrics.values()]):
                f.write("   ‚Ä¢ Menor RMSE entre todos os modelos\n")
            
            if abs(best_metrics['residuals_mean']) < best_metrics['residuals_std'] * 0.1:
                f.write("   ‚Ä¢ Res√≠duos bem centrados (sem vi√©s)\n")
            
            f.write("\nüöÄ PR√ìXIMOS PASSOS:\n")
            if best_metrics['r2'] < 0.9:
                f.write("   ‚Ä¢ Considerar ajustes nos hiperpar√¢metros\n")
                f.write("   ‚Ä¢ Avaliar engenharia de features\n")
            
            if abs(best_metrics['residuals_skew']) > 0.5:
                f.write("   ‚Ä¢ Investigar outliers nos dados\n")
                f.write("   ‚Ä¢ Considerar transforma√ß√µes nos dados\n")
            
            if not np.isnan(best_metrics['shapiro_p']) and best_metrics['shapiro_p'] <= 0.05:
                f.write("   ‚Ä¢ Verificar premissas do modelo\n")
                f.write("   ‚Ä¢ Considerar modelos n√£o-param√©tricos\n")
        else:
            # An√°lise de modelo √∫nico
            model_name = list(model_metrics.keys())[0]
            metrics = model_metrics[model_name]
            
            f.write(f"üìä AVALIA√á√ÉO DO MODELO: {model_name}\n\n")
            
            if metrics['r2'] > 0.9:
                f.write("‚úÖ VEREDICTO: Modelo EXCELENTE para uso em produ√ß√£o\n")
            elif metrics['r2'] > 0.8:
                f.write("‚úÖ VEREDICTO: Modelo BOM, adequado para uso\n")
            elif metrics['r2'] > 0.6:
                f.write("‚ö†Ô∏è VEREDICTO: Modelo MODERADO, necessita melhorias\n")
            else:
                f.write("‚ùå VEREDICTO: Modelo INADEQUADO, requer revis√£o completa\n")
            
            f.write("\nüéØ PONTOS FORTES:\n")
            if metrics['r2'] > 0.8:
                f.write("   ‚Ä¢ Boa capacidade de explica√ß√£o da variabilidade\n")
            if abs(metrics['residuals_mean']) < metrics['residuals_std'] * 0.1:
                f.write("   ‚Ä¢ Res√≠duos sem vi√©s significativo\n")
            if abs(metrics['residuals_skew']) < 0.5:
                f.write("   ‚Ä¢ Distribui√ß√£o de res√≠duos sim√©trica\n")
            
            f.write("\n‚ö†Ô∏è PONTOS DE ATEN√á√ÉO:\n")
            if metrics['r2'] < 0.8:
                f.write("   ‚Ä¢ R¬≤ relativamente baixo\n")
            if abs(metrics['residuals_mean']) >= metrics['residuals_std'] * 0.1:
                f.write("   ‚Ä¢ Poss√≠vel vi√©s nos res√≠duos\n")
            if abs(metrics['residuals_skew']) >= 0.5:
                f.write("   ‚Ä¢ Assimetria nos res√≠duos\n")
        
        f.write("\n")
        f.write("=" * 80 + "\n")
        f.write("FIM DO RELAT√ìRIO\n")
        f.write("=" * 80 + "\n")
    
    print_save_message(save_path, "Relat√≥rio textual")


def generate_quick_summary(results_dict, model_name=None):
    """
    Gera resumo r√°pido para visualiza√ß√£o no console
    
    Args:
        results_dict: Dicion√°rio com resultados dos modelos
        model_name: Nome do modelo (para an√°lise de modelo √∫nico)
    
    Returns:
        str: Resumo formatado
    """
    from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
    
    if model_name and isinstance(results_dict, dict) and 'actuals' in results_dict:
        # An√°lise de um √∫nico modelo
        single_model_results = {model_name: results_dict}
    else:
        # M√∫ltiplos modelos
        single_model_results = results_dict
    
    clean_results = validate_and_clean_metrics(single_model_results)
    
    if not clean_results:
        return "‚ùå Dados insuficientes para gerar resumo."
    
    summary = "\n" + "="*60 + "\n"
    summary += "üìä RESUMO R√ÅPIDO DE PERFORMANCE\n"
    summary += "="*60 + "\n"
    
    model_scores = []
    
    for model_name, results in clean_results.items():
        if 'actuals' in results and 'predictions' in results:
            actuals = np.array(results['actuals']).flatten()
            predictions = np.array(results['predictions']).flatten()
            
            r2 = r2_score(actuals, predictions)
            rmse = np.sqrt(mean_squared_error(actuals, predictions))
            mae = mean_absolute_error(actuals, predictions)
            
            # Score simples para ranking
            score = r2 * 100 - (rmse + mae) * 10
            model_scores.append((model_name, score, r2, rmse, mae))
            
            # Status baseado em R¬≤
            if r2 > 0.9:
                status = "üü¢ EXCELENTE"
            elif r2 > 0.8:
                status = "üü¢ BOM"
            elif r2 > 0.6:
                status = "üü° MODERADO"
            else:
                status = "üî¥ BAIXO"
            
            summary += f"\nüìà {model_name}:\n"
            summary += f"   Status: {status}\n"
            summary += f"   R¬≤: {r2:.6f} | RMSE: {rmse:.6f} | MAE: {mae:.6f}\n"
    
    # Ranking se houver m√∫ltiplos modelos
    if len(model_scores) > 1:
        model_scores.sort(key=lambda x: x[1], reverse=True)
        summary += "\nüèÜ RANKING:\n"
        for rank, (name, score, r2, rmse, mae) in enumerate(model_scores, 1):
            medal = get_medal_emoji(rank)
            summary += f"   {medal} {name} (Score: {score:.2f})\n"
        
        # Recomenda√ß√£o
        best_model = model_scores[0][0]
        summary += f"\nüéØ RECOMENDADO: {best_model}\n"
    
    summary += "\n" + "="*60 + "\n"
    
    return summary 